### 🧩 1. 先从普通的 Softmax 说起

假设我们有一个向量
$$
x = [x_1, x_2, \dots, x_n]
$$

Softmax 的定义是：
$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

也就是说，我们把所有分量先取指数，再除以所有指数的和，让结果变成一个概率分布。

---

### ⚠️ 2. 数值稳定性问题

如果 $x_i$ 很大（例如上百），那么 $e^{x_i}$ 可能会溢出（inf）。
常见的做法是引入一个 **平移技巧**（log-sum-exp trick）：

设
$$
m = \max_i x_i
$$

则
$$
\text{softmax}(x_i) = \frac{e^{x_i - m}}{\sum_{j=1}^{n} e^{x_j - m}}
$$

这样做不会改变结果（因为同时分子分母都乘了 $e^{-m}$），但可以避免指数爆炸。

---

### 🧠 3. 问题来了：如果 $n$ 特别大怎么办？

比如在 Transformer 的 attention 里，我们要计算：
$$
\text{softmax}(QK^\top)
$$
如果 $K$ 有几万个 token，就要同时计算所有 $e^{x_j}$ 并求和，非常耗显存。

所以——
我们希望 **能一边遍历数据一边计算 softmax，不用把所有 $e^{x_j}$ 都存起来。**
这就引出了 **Online Softmax** 算法。

---

### ⚙️ 4. Online Softmax 思想

目标是想“在线地”更新 softmax 的分母：
$$
S = \sum_{j=1}^{n} e^{x_j - m}
$$

但是我们不能提前知道 $m$（最大值），因为数据是流式到来的。
所以要在遍历过程中同时维护：

* 当前最大值：$m_i = \max(x_1, x_2, \dots, x_i)$
* 当前规范化的指数和：$s_i = \sum_{j=1}^{i} e^{x_j - m_i}$

关键在于，当新的元素 $x_{i+1}$ 到来时，如何更新这两个量。

---

### 💡 5. Online Softmax 更新公式

初始化：
$$
m_0 = -\infty, \quad s_0 = 0
$$

对每个新元素 $x_i$：

1️⃣ 更新最大值：
$$
m_i = \max(m_{i-1}, x_i)
$$

2️⃣ 更新规范化和：
$$
s_i = s_{i-1} \cdot e^{m_{i-1} - m_i} + e^{x_i - m_i}
$$

最后得到：
$$
\text{softmax}(x_k) = \frac{e^{x_k - m_n}}{s_n}
$$

---

### 🧩 6. 直觉理解

* 当新来的 $x_i$ 比旧最大值小：
  旧的 $m_{i-1}$ 不变，$e^{m_{i-1} - m_i} = 1$，很好理解。

* 当新来的 $x_i$ 比旧最大值大：
  我们必须把之前所有的 $e^{x_j - m_{i-1}}$ **重新缩放**，因为基准 $m_i$ 变大了。
  于是乘上 $e^{m_{i-1} - m_i}$。

这样就避免了重新扫描之前的所有数据，实现了 **单遍计算**。

---

### 🧮 7. 小总结

| 名称             | 需要存全部值？ | 是否数值稳定 | 复杂度      | 用途                           |
| -------------- | ------- | ------ | -------- | ---------------------------- |
| 普通 Softmax     | ✅ 是     | ⚠️ 需平移 | O(n)     | 小规模计算                        |
| Online Softmax | ❌ 否     | ✅ 稳定   | O(n)（单遍） | 大规模 / 流式输入（如 FlashAttention） |

---

## 使用分块的思想来加速计算SOFTMAX。

Online Softmax 的**核心思想**之一——它之所以叫 “online”，并不意味着必须“顺序”地从左到右算。关键在于它引入了一个**可组合的中间状态**。

我们来把这个想法正式化一下：

---

### 💡 1. 每个块都可以自己计算一个 `(m, s)`

对于任意一段数据（一个 block）
$$
X = [x_1, x_2, \dots, x_k]
$$
我们都可以用刚才那两个公式算出：

* 最大值
  $$
  m = \max_i x_i
  $$
* 平移后的指数和
  $$
  s = \sum_{i=1}^{k} e^{x_i - m}
  $$

也就是说，每个块只需要输出 **一个 pair：$(m, s)$**。
这两个值足以代表这段数据对 softmax 分母的“贡献”。

---

### ⚙️ 2. 两个块的组合公式（combine）

假设我们有两段：

* 块 A：$(m_A, s_A)$
* 块 B：$(m_B, s_B)$

它们的组合结果 $(m, s)$ 可以用：

$$
m = \max(m_A, m_B)
$$
$$
s = s_A \cdot e^{m_A - m} + s_B \cdot e^{m_B - m}
$$

这正是我们在“顺序更新”里用到的那个缩放关系。
只是现在我们把它推广到了任意两段的组合。

---

### 🧠 3. 意义：Softmax 的 **结合律**

这个组合操作是**可结合的**（associative）：

$$
\text{combine}(\text{combine}(A,B), C) = \text{combine}(A, \text{combine}(B,C))
$$

这意味着我们不必严格从左到右扫描，而可以：

* 分块并行计算每块的 $(m, s)$；
* 然后在树状结构中逐层合并。

这种并行合并在 GPU 上非常高效，是 **FlashAttention 和多核 softmax kernel** 的理论基础。

---

### 🧩 4. 最后得到的 Softmax

一旦得到了整体 $(m, s)$，我们只需：
$$
\text{softmax}(x_i) = \frac{e^{x_i - m}}{s}
$$

即每个元素都减去全局最大值 $m$ 并除以总和 $s$。
