### 核心内容摘要

深度学习中，激活函数为神经网络引入非线性，使其能够学习复杂模式。除了ReLU等传统函数，Swish、Hard-Swish和Hardshrink是具有特定用途的新型激活函数。

---

### 1. Swish 激活函数

Swish是一种由Google提出的平滑且非单调的激活函数，在许多深度网络中表现优于ReLU。

* **数学公式**:
  $Swish(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}}$
  其中，$\sigma(x)$ 是Sigmoid函数，$\beta$ 是一个常数或可学习的参数（通常默认为1）。当 $\beta=1$ 时，也称为SiLU。

* **核心特性**:
    * **平滑性**: 函数处处可导，有利于梯度下降的稳定性。
    * **非单调性**: 在负值区域存在先下降后上升的区间，这被认为能增强模型的表达能力。
    * **无上界有下界**: 无上界避免了梯度饱和，有下界则有助于正则化。
    * **缓解“死亡ReLU”**: 允许负值输入产生非零输出，减少了神经元失效的风险。

* **缺点**:
    * **计算成本高**: 包含指数运算，比ReLU的计算量更大。

* **应用场景**:
    * 适用于追求高模型精度的场景，尤其是在计算资源充足的深度网络中，如大型图像分类和自然语言处理模型。

---

### 2. Hard-Swish 激活函数

Hard-Swish是Swish函数的分段线性近似版本，旨在大幅提升计算效率，特别适用于移动和边缘设备。

* **数学公式**:
  $Hard-Swish(x) = x \cdot \frac{ReLU6(x + 3)}{6}$
  该公式使用`ReLU6`来近似Sigmoid函数，仅包含加、乘、比较等硬件友好型操作。

* **核心特性**:
    * **计算效率高**: 避免了指数运算，推理速度快，能耗低。
    * **性能接近Swish**: 在大多数应用中，其模型精度与Swish相当。
    * **易于部署**: 简单的数学结构使其在各种硬件平台上都易于实现和优化。

* **应用场景**:
    * 专为资源受限的环境设计，是现代轻量级神经网络（如MobileNetV3）的标准组件，广泛用于移动端和嵌入式设备。

#### Swish 与 Hard-Swish 对比

| 特性 | Swish | Hard-Swish |
| :--- | :--- | :--- |
| **公式** | $x \cdot \sigma(\beta x)$ | $x \cdot \frac{ReLU6(x + 3)}{6}$ |
| **平滑性** | 处处平滑 | 分段线性近似 |
| **计算成本** | 较高（指数运算） | 低（基本运算） |
| **主要应用** | 高性能模型、服务器端 | 高效模型、移动/边缘设备 |

