# CUDA FMA（融合乘加）学习手册

---

## 📖 第一章：什么是 FMA？

### 1.1 定义

**FMA** = **F**used **M**ultiply-**A**dd（融合乘加）

```c
__fmaf_rn(a, b, c)  ≡  a × b + c
```

### 1.2 函数命名规则

```
__fmaf_rn
  │││ ││
  │││ │└─ n = Nearest（最近）
  │││ └── r = Round（舍入）
  ││└──── f = float（单精度）
  │└───── fma = Fused Multiply-Add
  └────── __ = CUDA 内置函数前缀
```

### 1.3 舍入模式一览

| 后缀 | 含义 | 说明 |
|------|------|------|
| `_rn` | Round to Nearest | 四舍五入到最近偶数（默认/最常用） |
| `_rz` | Round towards Zero | 向零方向截断 |
| `_ru` | Round Up | 向正无穷舍入 |
| `_rd` | Round Down | 向负无穷舍入 |

---

## 📖 第二章：为什么需要舍入？

### 2.1 float 的内存结构

```
┌─────────┬──────────────┬─────────────────────────┐
│ 1 bit   │ 8 bits       │ 23 bits                 │
│ 符号位  │ 指数位       │ 尾数位（有效数字）      │
└─────────┴──────────────┴─────────────────────────┘
         32 位单精度浮点数（float）
```

**核心限制**：23 位尾数 ≈ 只能精确表示 **7 位十进制有效数字**

### 2.2 精度溢出问题

```
两个 7 位有效数字相乘：
1.234567 × 1.234567 = 1.52415765279089...
                      └──────14位──────┘

但 float 只能存 7 位 → 必须舍入！
```

### 2.3 关键结论

> ⚠️ **浮点运算的舍入不是可选的，而是必然发生的**
>
> 因为运算结果的精度往往超出存储容量

---

## 📖 第三章：FMA vs 普通运算

### 3.1 普通运算流程（两次舍入）

```
a * b + c

步骤 1: temp = a * b
        ↓
     [舍入] ← 精度损失①
        ↓
步骤 2: result = temp + c
        ↓
     [舍入] ← 精度损失②
        ↓
     最终结果
```

### 3.2 FMA 运算流程（一次舍入）

```
__fmaf_rn(a, b, c)

步骤 1: temp = a * b
        ↓
     [保持完整精度] ← 内部使用更多位数
        ↓
步骤 2: result = temp + c
        ↓
     [舍入] ← 仅此一次精度损失
        ↓
     最终结果
```

### 3.3 对比总结

| 特性 | 普通 `a*b+c` | `__fmaf_rn(a,b,c)` |
|------|--------------|---------------------|
| 舍入次数 | 2 次 | 1 次 |
| 精度 | 较低 | 较高 |
| 硬件指令数 | 2 条 | 1 条 |
| 性能 | 较慢 | 较快 |

---

## 📖 第四章：实际应用

### 4.1 典型使用场景

```cuda
// 矩阵乘法核心计算
r_c[tm][tn] = __fmaf_rn(r_comp_a[tm], r_comp_b[tn], r_c[tm][tn]);

// 等价于（但更精确、更快）：
// r_c[tm][tn] = r_comp_a[tm] * r_comp_b[tn] + r_c[tm][tn];
```

### 4.2 常见应用领域

- ✅ 矩阵乘法（GEMM）
- ✅ 深度学习前向/反向传播
- ✅ 科学计算
- ✅ 图形渲染
- ✅ 信号处理

### 4.3 相关函数家族

| 函数 | 数据类型 |
|------|----------|
| `__fmaf_rn` | float（单精度） |
| `__fma_rn` | double（双精度） |
| `__hfma` | half（半精度） |

---

## 📖 第五章：快速参考卡

```
┌─────────────────────────────────────────────────────┐
│           __fmaf_rn(a, b, c) 速查表                │
├─────────────────────────────────────────────────────┤
│  计算公式：  a × b + c                              │
│  数据类型：  float (32-bit)                         │
│  舍入方式：  四舍五入到最近偶数                     │
│  舍入次数：  1 次（优于普通运算的 2 次）            │
│  优势：      更高精度 + 更快速度                    │
│  硬件支持：  CUDA Compute Capability 2.0+           │
└─────────────────────────────────────────────────────┘
```

---

## 📝 要点回顾

1. **FMA 是什么**：一条硬件指令完成乘加运算
2. **为什么要舍入**：float 只有 23 位尾数，存不下运算的完整结果
3. **FMA 的优势**：中间结果不舍入，只在最后舍入一次
4. **`_rn` 的含义**：Round to Nearest，选择四舍五入的舍入策略
5. **使用场景**：高性能计算中追求精度和速度的核心运算